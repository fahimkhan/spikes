----------Intoduction to Hive-----------------

--MetaStore :
	Its relational DB
	Store details of meta data
	table dfinintion
	data type info
	By defaul derby DB

-- HiveQL
	Allows you to write sql like query

-- Driver
	Convert SQL like queries into number of Map and Reduce job

--Hive CLI
	Command line interface

-- Hives allow you to define schema on your data
-- Uses serializer and deserializer to read and write data


-- Hive WareHouse Principle
	Mata Data about all objects known to Hive, persisted in metastore
	Along side meta data it has Data

--Supports
	select
	limit
	distinct
	aliasing
	regex - to specific col (Not available in relational DB)
	interchangeable construct. from - select or select from
	Hive is not case sensitive
	; to terminate statement
	Sub query (But only in from clause)
	Union all (Union is not supported)

-- Create Database
	
	CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
	[COMMENT some_coment]
	[LOCATION hdfs_path]
	[WITH DBPROPERTIES(property_name=property_value..)] ;

	USE database_name;

	DROP (DATABASE|SCHEMA) [IF EXISTS] database_name;


-- Create Table

	CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
	[(col_name data_type [COMMENT col_comment],....)]
	[PARTITIONED BY (col_name data_type [COMMENT col_comment],....)]
	[ROW FORMAT row_format] [STORED AS file_format]
	[LOCATION hdfs_path]
	[TBLPROPERTIES(property_name=property_value..)];

-- For database it create directory as dbname.db Under this directory all the table will be created and that would be again a directory

-- truncate table tableName;

-- Overwrite data in table with new file

LOAD DATA INPATH '/user/directory-containing-file/' INTO  TABLE table_name;

-- LOCAL DATA INPATH
	moves data if data is in HDFS
	copy data if it is in local system

	Hence if you have data in HDFS then it is better to create an EXTERNAL table

-- To run bash command inside Hive shell use ! and then command

-- Create external table 

	CRAETE EXTERNAL TABLE(
		id int,
		age string
	)
	ROW FORMAT DELIMITED 
	FIELDS TERMINATED BY '|'
	STORED AS TEXTFILE
	LOCATION '/user/userinfo';

-- Three different command for describing table

	describe table_name;
	describe extended table_name;
	describe formatted table_name;

-- select * from table_name LIMIT 10; --do not run any map reduce job as it is not computing or filtering anything

-- Creating table and loading data from another table

	create table t2
	stored as RCFILE
	AS select count(*) cnt,id from t1 group by id;

-- We can create a skeleton of table without any data in it using other table

	create table t2 like t1;

	there wont be any data in t2. It just copy the structure of table t1


-----------------------------Hive Query Language-------------------------------------


-- Primitive Data Type

-NUMERIC
	TINYINT,INT,SMALLINT,BIGINT
	FLOAT
	DOUBLE
	DECIMAL  --from HIVE 0.11

-DATETIME
	TIMESTAMP
	DATE

- MISC
	BOOLEAN (TRUE or FALSE not 0/1)
	STRING
	BINARY

-- Complex/Collection Data type

	Arrays
		ARRAY<data_types>
	MAPS
		MAP<primitive_type,data_types> key has to be primitive data type
	STRUCT
		STRUCT<col_name:data_type,col_name1:data_type...>
	UNIONTYPE
		UNIONTYPE<data_type,data_type...> --ANy of the data_type


-- Type Conversion

	Smaller data type can be converted to bigger counter part

		Double - float,string,bigint
		float - int,smallint,tinyint
		string - timestamp

	Explicit Conversion using CAST

		CAST('13' AS INT)
		CAST('This result in NULL' AS INT) -- ANything doesnt cast properly result in NULL


-- Partiotion Table

	Managed Partiotion Table

		create table page_views(event_time STRING,userid STRING,page STRING)
		PARTITIONED BY(dt STRING,applicationtype STRING)  -- This two are virtual column
		STORED AS TEXTFILE;

		Directory Strucure After creating table
			- page_views
				- dt
					-applicationtype

		You can overwrite the partition data but you have to specify the partition

	External Partitioned Table

		create external table page_views(event_time STRING,userid STRING,page STRING)
		PARTITIONED BY(dt STRING,applicationtype STRING)  -- This two are virtual column
		STORED AS TEXTFILE;

		ALTER TABLE page_views ADD PARTITION(dt='2012-09-09',applicationtype='Linux')
		LOCATION '/somewhere/'

		Directory Structure is upto you

-- EXPLAIN COMMAND
	Use for query optimization. It give the flow hive will follow on your query

	EXPLAIN select * from page_views;

-- Multiple Insert

	from movies
	insert overwrite table horro_movies select * where type='horror'
	insert into table action_movies select * where type='action';

-- Dynamic Partition Insert

	Dynamically determine which partition to create and populate
	Use input data to determine partition

		from page_view src
		INSERT OVERWRITE table view_stg PARTITION(applicationtype='Web',dt,page)
		select src.event,src.userid,src.dt,src.page where applicationtype='Web';

	Default maximum dynamic partition = 1000


-- Data Retrieval
	select a ,sum(c) from t1 group by a;

	Grouping Set

		select a,b,sum(c) from t1 group by a,b grouping sets((a,b),a);
		
			equivalent to
		
		select a,b,sum(c) from t1 group by a,b
		union all 
		select a,NULL,sum(c) from t1 group by a;

	CUBE
		select a,b,c ,sum(d) from t1 group by a,b,c WITH CUBE ---grouping set will be all possible combination

	ROLLUP
		select a,b,c ,sum(d) from t1 group by a,b,c WITH ROLLUP  --based on heirarchy

		grouping set -- ((a,b,c),(a,b),a,())


-- Built In Function
	Mathematical
		rand()
		pow()
		abs()
		floor()
		round()
		ceil()
		tan()

	collection
		size(Map<K.V>)
		map_keys(Map<K.V>)
		map_values(Map<K.V>)

		select array_contains(a,'test') from t1;

	Date
		unix_timestamp()
		year(string d), month(string d), day(string d),hour,second
		datediff(string enddate,string startdate)
		date_add(string startdate,int days)
		date_sub(string startdate,int days)
		to_date(string timestamp)

	Conditional
		select (a=b,'true result','flase_result') from t1;
		select COALESCE(a,b,c) from t1 --- return first non null value;
		select CASE a when 123 then 'first' when 234 the 'second' else 'None' END from t1; 

	string
		concat(a,b)
		concat_ws(sep,a,b) --with separator
		regex_replace("Hive","ive","adoop")
		substr(string|binary A,int start)
		substring(string|binary A,int start,int length)
		sentences("These is used for tokenization") -- Result would be array


-- Built in UDAF
	count(*)
	count(1)
	count(expr) --null ommited
	count(DISTINCT expr)
	SUM(col)
	SUM(DISTINCT col)
	AVG,MIN,MAX,VARIANCE,STDDEV_POP

Having clause for group by 
	having SUM(c) > 2



--Sorting in Hive

	Order BY
		select x,y from t1 order by x ASC;

	SORT BY
		can run on multiple reducer but doesnot guarantee global sort but in specific reducer it is in sorted order

		select x,y from t1 sort by x;

--Controlling Data Flow
	
	DISTRIBUTE BY

		select x,y,z from t1 distribute by y;  --Here it is partition of map reduce to send data to proper reducer

		select x,y,z from t1 distribute by y sort by x;  
		-- if your distribute and sort has same column then use cluster by clause
 
	CLUSTER BY
		select x,y,z from t1 CLUSTER by y;

-- Hive CLI

	batch mode
		hive -e 'Your query';
		hive -S -e 'Your query' > result.txt  ---S is silent
		hive -f get_data.sql


-- Variable substitution
	4 namespace

	- hivevar
		-d,--define,--hivevar
		set hivevar:name = value

	- hiveconf
		--hiveconf
		set hiveconf:property = value

	- system
		set system:property = value

	- env
		set env:property = value

	use ${} to access the value


----------------------------- Advanced HQL---------------------------------

-- Bucketing
	It also spilt the data
	More efficient
	Better performance for Map Side Join
	Used with partition or w/o partitioned when partitioned does not work for you data

	Bucket can also be sorted
	Sort Merge Bucket Join(SMB)

		ex.
		Create table t1(a INT,b STRING,c STRING)
		CLUSTERED BY(b) INTO 256 BUCKETS;

		It run the has function against the column you gave in this 'b' and splits it into bucket

		ex.
		CREATE table t1(a INT, b STRING,c STRING)
		PARTITIONED BY (dt STRING)
		CLUSTERED BY (b) SORTED BY(c) INTO 64 BUCKETS

	Hive doesn't enforce bucketing on data loaded into table. User has to explicitly make it happen
	2 Approach
		1.
			set mapred.reduce.task = 64;
			INSERT OVERWRITE TABLE t1
			SELECT a,b,c from t2 CLUSTER BY b;

		2. 
			set hive.enforce.bucketing = true;
			INSERT OVERWRITE table t1 
			SELECT a,b,c from t2;

			Number of reducer and hence the number of output files equal the number of buckets


-- Bucket Sampling
	Can be done on any table

		select * from source TABLESAMPLE(BUCKET x OUT OF y[ON colName] );

		ex .
			CREATE TABLE page_views(userid INT,page STRING,views INT)
			PARTITIONED BY(dt STRING)
			CLUSTERED BY(userid) SORTED BY(dt) INTO 64 BUCKETS;


			SELECT * from page_views TABLESAMPLE(BUCKET 3 OUT OF 64 ON userid);
			SELECT * from page_views TABLESAMPLE(BUCKET 3 OUT OF 64 ON rand());


-- Block Sampling
	based on HDFS block.
	percentage of data size(notice this is not # of row)
	Returns atleast percentage specified
	Doesnt always work
		Depends on compression and input format

	ex 
		select * from source TABLESAMPLE(n PERCENT);
		select * from source TABLESAMPLE(xM);  --Xmegbyte


-- Joins

	Join Types
		JOIN(Inner Join)
		LEFT,RIGHT,FULL[OUTER] JOIN
		LEFT SEMI JOIN
		CROSS JOIN

	Equality JOIN only (equal sign only)

	Multiple table can be joined

	Inner JOIN
		select a.val,b.val FROM a JOIN b ON(a.key=b.key); 

	LEFT OUTER Join
		select a.val,b.val,c.val from a LEFT OUTER JOIN b ON(a.key=b.key) JOIN c ON(c.key=a.key);  --Start from left

	LEFT SEMI JOIN (For IN and EXISTS --as it is not supported in Hive)

		SELECT a.val from a LEFT SEMI JOIN b ON(a.key=b.key)

	CROSS JOIN
		select a.*,b.* from a CROSS JOIN b;



-- Joins In depth

	STREMABLE
		One Table is fixed(small) and store in memmory and other table is streamed though it

	MAP SIDE JOIN
		ALL table are small enuf to fit into memmory except 1 which is streamed through mapper
		HASH table is used
		No Full or Right Outer join possible. No Union Possible

		first appoach
			SELECT MAP JOIN(b) a.*,b.* from a JOIN b ON(a.key=b.key)

		second approach Hive do it self
			set hive.auto.convert.join = true;
			SELECT a.*,b.* from a
			JOIN b ON(a.key=b.key);


--Map Side Join Bucketed Tables
	Buckets can be joined with each other (Map-side Join) when:
		Tables being joined are bucketed on join columns (Clustered)
		Number of buckets in one table is a multiple of the number of buckets in the other table
		Set hive.optimize.bucketmapjoin=true


	Sort Merge Join
		Tables being joined are bucketed on join columns (Clustered)
		They have the same number of buckets
		Buckets are also sorted
		Set:
			hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
			hive.optimize.bucketmapjoin = true;
			hive.optimize.bucketmapjoin.sortedmerge = true;

-- Distributed Cache

	An approach used by MapReduce to distribute files across data nodes
	Provides a means for data nodes to access files local to the data node itself (cached copy)
	Typically used with
		Text files
		Archives (compressed files)
		Jars and other program files
	
	Used to distribute hash archive of a table for Map-Side joins

	ADD FILE mydata.txt;
	ADD ARCHIVE sendme.zip;
	ADD JAR myprogram.jar;
	LIST FILES|JARS|ARCHIVES [filepath];


------Advance Hive Function


	--Table Generating Function


		Explode()
			Takes array as input
			No other expressions allowed in SELECT 
			Can’t be nested
			GROUP BY / CLUSTER BY / DISTRIBUTE BY / SORT BY not supported
			Explodes elements of array as separate rows

			select explode(actor) from movies;


		Lateral View
			Takes UDTF function as input
			Provides virtual table for accessing combined results
			
			SELECT a, b, columnAlias
			FROM baseTable
			LATERAL VIEW UDTF(expression) tableAlias AS columnAlias;

			SELECT a, b, col1, col2
			FROM baseTable
			LATERAL VIEW UDTF(x) t1 AS col1
			LATERAL VIEW UDTF(col1) t2 AS col2;


			ex.
				SELECT movie_id, title, actor
				FROM movies LATERAL VIEW explode(actors) actorTable AS actor;

		Outer Lateral Views(for getting NULL columan as well)
			SELECT movie_id, title, actor
			FROM movies LATERAL VIEW OUTER explode(actors) actorTable AS actor;

	--Extending Hive

		Steps
			Create Program
			Provide Packaged Program to Hive
			Create Temporary Function XYZ


		Import Section
			import org.apache.hadoop.hive.ql.exec.UDF;
			import org.apache.hadoop.hive.ql.exec.Description;
		Anything you need as part of your UDF
			import org.apache.hadoop.io.Text
			import java.util.*;

		Add annotations
			Description, Deterministic, Stateful, DistinctLike

		Extend the UDF class


		Provide an implementation of the evaluate function possibly with multiple overloads


		Compile and package code

		Tell Hive about the JAR file. Use ADD JAR /path/to/jar/myudf.jar
		Adds JAR to distributed cache & classpath
		Create TEMPORARY FUNCTION and reference class


		What about that TEMPORARY function
			Function only exists in current user’s session
			Use the -i option when launching hive from the command line
				Provide an initialization file
			Use the .hiverc file
				User’s home directory
				Hive’s bin directory /usr/lib/hive/bin/


	-- Hadoop Streaming

		TRANSFORM
			SELECT TRANSFORM (col1 [,col2... coln])
			USING 'Code File|Program' [AS (list of columns [and casts])]
			FROM SourceTable;

			Columns are sent as tab separated string (default)
			Null values are replaced with literal “\N”
			
			Specifying list of output columns is optional, if not provided:

				First column is the key
				Remaining string is the value, even if there are multiple tabs (columns)
				Key column referenced using key

			ex.
				SELECT TRANSFORM (movie_title)
				USING '/bin/sed "s/[^ ][^ ]*/(&)/g"' AS movie_title_parantheses
				FROM movies;



		MAP,REDUCE


	-- Windowing and Analytic Function

		LEAD/LAG
		LAST_VALUE
		FIRST_VALUE
		PARTITION BY
		OVER CLAUSE
		WINDOW clause to provide window specification
		RANK, ROW_NUMBER, DENSE_RANK
		CUME_DIST, PERCENT_RANK, NTILE



------------Storage And Eco System in Hive--------------------














