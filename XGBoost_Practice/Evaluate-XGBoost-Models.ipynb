{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate XGBoost Models\n",
    "The goal of developing a predictive model is to develop a model that is accurate on unseen data.\n",
    "This can be achieved using statistical techniques where the training dataset is carefully used\n",
    "to estimate the performance of the model on new and unseen data. In this tutorial you will\n",
    "discover how you can evaluate the performance of your gradient boosting models with XGBoost\n",
    "in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models With Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.95%\n"
     ]
    }
   ],
   "source": [
    "# train-test split evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models With k-Fold Cross Validation\n",
    "Cross validation is an approach that you can use to estimate the performance of a machine\n",
    "learning algorithm with less variance than a single train-test set split. It works by splitting the\n",
    "dataset into k-parts (e.g. k=5 or k=10). Each split of the data is called a fold. The algorithm\n",
    "is trained on k-1 folds with one held back and tested on the held back fold. This is repeated\n",
    "so that each fold of the dataset is given a chance to be the held back test set. After running\n",
    "cross validation you end up with k-different performance scores that you can summarize using a\n",
    "mean and a standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.69% (7.11%)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# CV model\n",
    "model = XGBClassifier()\n",
    "kfold = KFold(n=len(X), n_folds=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have many classes for a classification type predictive modeling problem or the classes\n",
    "are imbalanced (there are a lot more instances for one class than another), it can be a good idea to\n",
    "create stratified folds when performing cross validation. This has the effect of enforcing the same\n",
    "distribution of classes in each fold as in the whole training dataset when performing the cross\n",
    "validation evaluation. The scikit-learn library provides this capability in the StratifiedKFold\n",
    "class. Below is the same example modified to use stratified cross validation to evaluate an\n",
    "XGBoost model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.95% (5.88%)\n"
     ]
    }
   ],
   "source": [
    "# stratified k-fold cross validation evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# CV model\n",
    "model = XGBClassifier()\n",
    "kfold = StratifiedKFold(Y, n_folds=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
