{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for gradient boosting\n",
    "XGBoost is a popular implementation of Gradient Boosting because of its speed and performance.\n",
    "Internally, XGBoost models represent all problems as a regression predictive modeling problem\n",
    "that only takes numerical values as input. If your data is in a different form, it must be prepared\n",
    "into the expected format. In this tutorial you will discover how to prepare your data for using\n",
    "with gradient boosting with the XGBoost library in Python. After reading this tutorial you will\n",
    "know:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode String Class Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# multiclass classification\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Load Data\n",
    "data = read_csv('iris.csv',header=0)\n",
    "dataset = data.values\n",
    "\n",
    "# split data into X and y\n",
    "X = dataset[:,0:4]\n",
    "Y = dataset[:,4]\n",
    "\n",
    "# encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "\n",
    "\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, label_encoded_y,\n",
    "test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode Categorical Data\n",
    "Some datasets only contain categorical data, for example the breast cancer dataset. This dataset\n",
    "describes the technical details of breast cancer biopsies and the prediction task is to predict\n",
    "whether or not the patient has a recurrence of cancer, or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost may assume that encoded integer values for each input variable have an ordinal\n",
    "relationship. For example that left-up encoded as 0 and left-low encoded as 1 for the\n",
    "breast-quad variable have a meaningful relationship as integers. In this case, this assumption\n",
    "is untrue. Instead, we must map these integer values onto new binary variables, one new variable\n",
    "for each categorical value. For example, the breast-quad variable has the values:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "left-up\n",
    "left-low\n",
    "right-up\n",
    "right-low\n",
    "central\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can model this as 5 binary variables as follows:\n",
    "\n",
    "left-up,    \n",
    "1,0,0,0,0\n",
    "left-low,\n",
    "0,1,0,0,0\n",
    "right-up,\n",
    "0,0,1,0,0\n",
    "right-low,\n",
    "0,0,0,1,0\n",
    "central\n",
    "0,0,0,0,1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called one hot encoding. We can one hot encode all of the categorical input variables\n",
    "using the OneHotEncoder class in scikit-learn. We can one hot encode each feature after we\n",
    "have label encoded it. First we must transform the feature array into a 2-dimensional NumPy\n",
    "array where each integer value is a feature vector with a length 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X shape: : ', (285, 43))\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Accuracy: 69.47%\n"
     ]
    }
   ],
   "source": [
    "# binary classification, breast cancer dataset, label and one hot encoded\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# load data\n",
    "data = read_csv('datasets-uci-breast-cancer.csv', header=0)\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:9]\n",
    "Y = dataset[:,9]\n",
    "# encode string input values as integers\n",
    "columns = []\n",
    "for i in range(0, X.shape[1]):\n",
    "    label_encoder = LabelEncoder()\n",
    "    feature = label_encoder.fit_transform(X[:,i])\n",
    "    feature = feature.reshape(X.shape[0], 1)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    columns.append(feature)\n",
    "    \n",
    "# collapse columns into array\n",
    "encoded_x = numpy.column_stack(columns)\n",
    "\n",
    "print(\"X shape: : \", encoded_x.shape)\n",
    "# encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_x, label_encoded_y,\n",
    "test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support for Missing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost can automatically learn how to best handle missing data. In fact, XGBoost was\n",
    "designed to work with sparse data, like the one hot encoded data from the previous section, and\n",
    "missing data is handled the same way that sparse or zero values are handled, by minimizing the\n",
    "loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Accuracy: 83.84%\n"
     ]
    }
   ],
   "source": [
    "# binary classification, missing data\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load data\n",
    "dataframe = read_csv(\"horse-colic.csv\", delim_whitespace=True, header=None)\n",
    "\n",
    "dataset = dataframe.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:27]\n",
    "Y = dataset[:,27]\n",
    "# set missing values to 0\n",
    "X[X == '?'] = 0\n",
    "\n",
    "# convert to numeric\n",
    "X = X.astype('float32')\n",
    "# encode Y class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y,\n",
    "test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can tease out the effect of XGBoost’s automatic handling of missing values, by marking\n",
    "the missing values with a non-zero value, such as 1.\n",
    "\n",
    "X[X == '?'] = 1\n",
    "\n",
    "We can also impute the missing data with a specific value. It is common to use a mean or a\n",
    "median for the column. We can easily impute the missing data using the scikit-learn Imputer\n",
    "class.\n",
    "\n",
    "# impute missing values as the mean\n",
    "imputer = Imputer()\n",
    "imputed_x = imputer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
